name: Local LM Studio Assistant
version: 1.0.0
schema: v1

models:
  # Agent-capable chat/edit/apply model (Qwen coder)
  - name: Qwen2.5-Coder 14B (LM Studio)
    provider: lmstudio
    model: qwen2.5-coder-14b-instruct
    apiBase: http://host.docker.internal:9999/v1
    roles: [chat, edit, apply]
    # Ensure Agent Mode tool-calls are enabled (LM Studio is OpenAI-compatible, but we make it explicit)
    capabilities: [tool_use]

  # Secondary chat/edit model (no apply/agent)
  - name: Llama 3.1 8B (LM Studio)
    provider: lmstudio
    model: meta-llama-3.1-8b-instruct
    apiBase: http://host.docker.internal:9999/v1
    roles: [chat, edit]

  # Fast inline autocomplete (optional)
  - name: Qwen2.5-Coder 7B (Autocomplete)
    provider: lmstudio
    model: qwen2.5-coder-7b-instruct
    apiBase: http://host.docker.internal:9999/v1
    roles: [autocomplete]

  # Embeddings for @codebase search (LM Studio supports an embeddings endpoint)
  - name: Nomic Embed (LM Studio)
    provider: lmstudio
    model: nomic-ai/nomic-embed-text-v1.5-GGUF
    apiBase: http://host.docker.internal:9999/v1
    roles: [embed]

context:
  - provider: code
  - provider: docs
  - provider: diff
  - provider: terminal
  - provider: problems
  - provider: folder
  - provider: codebase